{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Setup"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:20:13.504644Z",
     "start_time": "2024-06-14T15:20:13.388961Z"
    }
   },
   "cell_type": "code",
   "source": "!whoami",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureuser\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:25:22.959159Z",
     "start_time": "2024-06-14T15:25:04.537947Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install tensorflow-gpu==2.3.0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting tensorflow-gpu==2.3.0\r\n",
      "  Downloading tensorflow_gpu-2.3.0-cp38-cp38-manylinux2010_x86_64.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (1.6.3)\r\n",
      "Collecting gast==0.3.3 (from tensorflow-gpu==2.3.0)\r\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (0.2.0)\r\n",
      "Collecting h5py<2.11.0,>=2.10.0 (from tensorflow-gpu==2.3.0)\r\n",
      "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (1.1.2)\r\n",
      "Collecting numpy<1.19.0,>=1.16.0 (from tensorflow-gpu==2.3.0)\r\n",
      "  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl.metadata (2.1 kB)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (3.3.0)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (3.19.6)\r\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (2.9.1)\r\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0 (from tensorflow-gpu==2.3.0)\r\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (2.4.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (1.16.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow-gpu==2.3.0) (0.34.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow-gpu==2.3.0) (1.14.0)\r\n",
      "Collecting scipy==1.4.1 (from tensorflow-gpu==2.3.0)\r\n",
      "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./.local/lib/python3.8/site-packages (from tensorflow-gpu==2.3.0) (1.64.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.30.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.4.6)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.6)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (70.0.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.6.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.8.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.0.3)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (5.3.3)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.2.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.0.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (4.11.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.8)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.25.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2019.11.28)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.1.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./.local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.19.2)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.1.0)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.4.2)\r\n",
      "Downloading tensorflow_gpu-2.3.0-cp38-cp38-manylinux2010_x86_64.whl (320.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m320.5/320.5 MB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\r\n",
      "Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.0/26.0 MB\u001B[0m \u001B[31m80.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.9/2.9 MB\u001B[0m \u001B[31m120.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.6/20.6 MB\u001B[0m \u001B[31m104.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m459.0/459.0 kB\u001B[0m \u001B[31m41.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tensorflow-estimator, numpy, gast, scipy, h5py, tensorflow-gpu\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\r\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.23.1\r\n",
      "    Uninstalling numpy-1.23.1:\r\n",
      "      Successfully uninstalled numpy-1.23.1\r\n",
      "  Attempting uninstall: gast\r\n",
      "    Found existing installation: gast 0.4.0\r\n",
      "    Uninstalling gast-0.4.0:\r\n",
      "      Successfully uninstalled gast-0.4.0\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.10.1\r\n",
      "    Uninstalling scipy-1.10.1:\r\n",
      "      Successfully uninstalled scipy-1.10.1\r\n",
      "  Attempting uninstall: h5py\r\n",
      "    Found existing installation: h5py 3.11.0\r\n",
      "    Uninstalling h5py-3.11.0:\r\n",
      "      Successfully uninstalled h5py-3.11.0\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pandas 2.0.3 requires numpy>=1.20.3; python_version < \"3.10\", but you have numpy 1.18.5 which is incompatible.\r\n",
      "seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.18.5 which is incompatible.\r\n",
      "tensorflow 2.9.1 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\r\n",
      "tensorflow 2.9.1 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.3.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed gast-0.3.3 h5py-2.10.0 numpy-1.18.5 scipy-1.4.1 tensorflow-estimator-2.3.0 tensorflow-gpu-2.3.0\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install tensorflow[and-cuda]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:19:04.613040Z",
     "start_time": "2024-06-14T15:19:02.599010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install numpy==1.23.1 Pillow==9.2.0 importlib-metadata==4.11.3 matplotlib==3.5.2 scikit-learn==1.1.1 tensorboard==2.9.1 jupyter==1.0.0 ipykernel==6.9.1 seaborn==0.13.2"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:26:44.755213Z",
     "start_time": "2024-06-14T15:26:44.737656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "sys.path.append('/home/azureuser/projects/opencampus-preconditioner-ai-project')\n",
    "\n",
    "# Change the working directory to the project directory\n",
    "os.chdir('/home/azureuser/projects/opencampus-preconditioner-ai-project')\n",
    "print(os.getcwd())  # Verify the change"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/azureuser/opencampus-preconditioner-ai-project'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/home/azureuser/opencampus-preconditioner-ai-project\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Change the working directory to the project directory\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/home/azureuser/opencampus-preconditioner-ai-project\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(os\u001B[38;5;241m.\u001B[39mgetcwd())  \u001B[38;5;66;03m# Verify the change\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/azureuser/opencampus-preconditioner-ai-project'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:19:09.629323Z",
     "start_time": "2024-06-14T15:19:07.550576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check wd\n",
    "current_base_path = os.getcwd()\n",
    "print(f\"Path to current working directory: {current_base_path}\")\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse.linalg import gmres\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# import matrixlib\n",
    "import matrixlib.io\n",
    "import matrixlib.util\n",
    "import matrixlib.plot\n",
    "import matrixlib.generate\n",
    "from matrixlib.metadata import MatrixMetadata\n",
    "\n",
    "#import CNN\n",
    "import CNN.models\n",
    "import CNN.training\n",
    "import CNN.evaluation\n",
    "\n",
    "# import solvers\n",
    "#import solvers.gmres\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to current working directory: /home/azureuser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 15:19:07.735161: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-14 15:19:07.738995: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-14 15:19:07.739006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matrixlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 17\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# import matrixlib\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatrixlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatrixlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatrixlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplot\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'matrixlib'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:41:20.991326Z",
     "start_time": "2024-06-14T15:41:20.987593Z"
    }
   },
   "cell_type": "code",
   "source": "tf.config.list_physical_devices('GPU') ",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generate Synthetic Data\n",
    "Generate a set of `n` square symmetrical and positive semi matrices of dimensions `MATRIX_DIM` to RAM.\n",
    "\n",
    "## Define Constants\n",
    "The following constants are used throughout the rest of the notebook."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:26:16.357468Z",
     "start_time": "2024-06-14T15:26:16.353799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MATRIX_DIM = 64\n",
    "NUMBER_OF_MATRICES = 1000\n",
    "\n",
    "AVERAGE_BLOCK_SIZE = 10\n",
    "BLOCK_SIZE_STD_DEV = 0.66\n",
    "\n",
    "NOISE_BACKGROUND_DENSITY_RANGE = (0.3, 0.5)\n",
    "NOISE_BACKGROUND_VALUE_RANGE = (0.0, 0.5)\n",
    "\n",
    "NOISE_BLOCK_GAP_CHANCE = 0.5\n",
    "NOISE_BLOCK_SIZE_RANGE = (3, 32)\n",
    "NOISE_BLOCK_DENSITY_RANGE = (0.3, 0.5)\n",
    "NOISE_BLOCK_VALUE_RANGE = (0.0, 1.0)\n",
    "\n",
    "TRUE_BLOCK_GAP_CHANCE = 0.0\n",
    "TRUE_BLOCK_SIZE_RANGE = (2, 32)\n",
    "TRUE_BLOCK_DENSITY_RANGE = (0.5, 0.7)\n",
    "TRUE_BLOCK_VALUE_RANGE = (0.0, 1.0)\n",
    "\n",
    "DIAGONAL_BAND_RADIUS = 10\n",
    "\n",
    "RNG_SEED = 42"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:26:18.656659Z",
     "start_time": "2024-06-14T15:26:18.652253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_matrices(number_of_matrices: int, dimension: int) -> (np.ndarray, np.ndarray):\n",
    "    generated_metadata: MatrixMetadata = MatrixMetadata(NUMBER_OF_MATRICES, MATRIX_DIM)\n",
    "    generated_matrices: np.array = matrixlib.generate.__init_zero_matrices(number_of_matrices, dimension)\n",
    "\n",
    "    # add background noise to the generated matrices\n",
    "    matrixlib.generate.add_noise(\n",
    "        generated_matrices,\n",
    "        generated_metadata.noise_background_density,\n",
    "        NOISE_BACKGROUND_DENSITY_RANGE,\n",
    "        NOISE_BACKGROUND_VALUE_RANGE\n",
    "    )\n",
    "\n",
    "    # add noise blocks with higher values to the diagonal of the matrix\n",
    "    noise_block_sizes = matrixlib.generate.add_blocks(\n",
    "        generated_matrices,\n",
    "        None,\n",
    "        generated_metadata.noise_block_density,\n",
    "        NOISE_BLOCK_DENSITY_RANGE,\n",
    "        NOISE_BLOCK_VALUE_RANGE,\n",
    "        NOISE_BLOCK_SIZE_RANGE,\n",
    "        NOISE_BLOCK_GAP_CHANCE,\n",
    "        AVERAGE_BLOCK_SIZE,\n",
    "        BLOCK_SIZE_STD_DEV,\n",
    "    )\n",
    "\n",
    "    # add 'real' blocks to the diagonal of the matrix without gaps\n",
    "    true_block_sizes = matrixlib.generate.add_blocks(\n",
    "        generated_matrices,\n",
    "        generated_metadata.block_starts,\n",
    "        generated_metadata.true_block_density,\n",
    "        TRUE_BLOCK_DENSITY_RANGE,\n",
    "        TRUE_BLOCK_VALUE_RANGE,\n",
    "        TRUE_BLOCK_SIZE_RANGE,\n",
    "        TRUE_BLOCK_GAP_CHANCE,\n",
    "        AVERAGE_BLOCK_SIZE,\n",
    "        BLOCK_SIZE_STD_DEV,\n",
    "    )\n",
    "\n",
    "    return generated_matrices, generated_metadata"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:26:20.121762Z",
     "start_time": "2024-06-14T15:26:20.090742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_matrices, metadata = generate_matrices(NUMBER_OF_MATRICES, MATRIX_DIM)\n",
    "matrices = generated_matrices.reshape(NUMBER_OF_MATRICES, MATRIX_DIM, MATRIX_DIM, 1)\n",
    "labels = metadata.block_starts\n",
    "\n",
    "print(matrices.shape)\n",
    "\n",
    "# Create dataset from matrices and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((matrices, labels))\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * NUMBER_OF_MATRICES)\n",
    "val_size = int(0.1 * NUMBER_OF_MATRICES)\n",
    "test_size = NUMBER_OF_MATRICES - train_size - val_size\n",
    "\n",
    "train_dataset = dataset.take(train_size).shuffle(buffer_size=10)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "test_dataset = dataset.skip(train_size + val_size)\n",
    "\n",
    "print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MatrixMetadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m generated_matrices, metadata \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_matrices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNUMBER_OF_MATRICES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMATRIX_DIM\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m matrices \u001B[38;5;241m=\u001B[39m generated_matrices\u001B[38;5;241m.\u001B[39mreshape(NUMBER_OF_MATRICES, MATRIX_DIM, MATRIX_DIM, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m labels \u001B[38;5;241m=\u001B[39m metadata\u001B[38;5;241m.\u001B[39mblock_starts\n",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m, in \u001B[0;36mgenerate_matrices\u001B[0;34m(number_of_matrices, dimension)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_matrices\u001B[39m(number_of_matrices: \u001B[38;5;28mint\u001B[39m, dimension: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m (np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[0;32m----> 2\u001B[0m     generated_metadata: MatrixMetadata \u001B[38;5;241m=\u001B[39m \u001B[43mMatrixMetadata\u001B[49m(NUMBER_OF_MATRICES, MATRIX_DIM)\n\u001B[1;32m      3\u001B[0m     generated_matrices: np\u001B[38;5;241m.\u001B[39marray \u001B[38;5;241m=\u001B[39m matrixlib\u001B[38;5;241m.\u001B[39mgenerate\u001B[38;5;241m.\u001B[39m__init_zero_matrices(number_of_matrices, dimension)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# add background noise to the generated matrices\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'MatrixMetadata' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verfiy Shape\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "# for element in val_dataset:\n",
    "#    print(element)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# reload the library when changes were made to it\n",
    "importlib.reload(matrixlib.io)\n",
    "importlib.reload(matrixlib.util)\n",
    "importlib.reload(matrixlib.plot)\n",
    "importlib.reload(matrixlib.generate)\n",
    "#importlib.reload(CNN.dataloader)\n",
    "\n",
    "importlib.reload(CNN.models)\n",
    "importlib.reload(CNN.training)\n",
    "importlib.reload(CNN.evaluation)\n",
    "importlib.reload(matrixlib.generate)\n",
    "\n",
    "importlib.reload(solvers.gmres)\n",
    "importlib.reload(solvers.blockjacobi)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define parameters\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001)\n",
    "num_epochs = 100\n",
    "log_dir = \"runs/matrix_cnn_experiment\"\n",
    "\n",
    "# Check log files\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "print(\"Files in log directory:\", os.listdir(log_dir))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compile Model\n",
    "model = CNN.models.Baseline(input_shape=(MATRIX_DIM, MATRIX_DIM, 1))\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Start Training Loop\n",
    "trained_model, train_losses, val_losses = CNN.training.train_model(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset.batch(8),  # Adjust when connected to vm\n",
    "    val_dataset=val_dataset.batch(8),\n",
    "    loss_fn=loss_fn, \n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    log_dir=log_dir\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot losses\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Plots the training and validation losses.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list of float): List of training losses per epoch.\n",
    "        val_losses (list of float): List of validation losses per epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Restore the model weights from the checkpoint\n",
    "trained_model = CNN.models.Baseline(input_shape=(MATRIX_DIM, MATRIX_DIM, 1))\n",
    "\n",
    "checkpoint_path = \"runs/matrix_cnn_experiment/best_model_checkpoint\"\n",
    "checkpoint = tf.train.Checkpoint(model=trained_model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path)).expect_partial()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluation_results = CNN.evaluation.evaluate_model(\n",
    "    model=trained_model,\n",
    "    test_dataset=test_dataset.batch(8),\n",
    "    loss_fn=loss_fn\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract test matrices and labels from dataset\n",
    "test_matrices = generated_matrices[train_size + val_size:]\n",
    "test_labels = metadata.block_starts[train_size + val_size:]\n",
    "\n",
    "# Make Predictions\n",
    "test_predictions = trained_model.predict(test_dataset.batch(8))\n",
    "\n",
    "# Convert to Binary\n",
    "threshold = 0.5\n",
    "binary_predictions = (test_predictions >= threshold).astype(int)\n",
    "\n",
    "# Calculate Metrics\n",
    "overall_accuracy = (binary_predictions == test_labels).mean()\n",
    "print(\"Overall Accuracy:\")\n",
    "print(overall_accuracy)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Block Jacobi Preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(test_matrices.shape)\n",
    "print(binary_predictions.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def block_jacobi_preconditioner_from_predictions(A, binary_predictions):\n",
    "    \"\"\"\n",
    "    Calculates the block Jacobi preconditioner for each 2D matrix in a 3D array using binary predictions for block starts.\n",
    "\n",
    "    Args:\n",
    "        A: NumPy array of shape (n, m, m) representing n square matrices of size m x m.\n",
    "        binary_predictions: NumPy array of shape (n, m) where each row indicates block starts with 1s.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of the same shape as A representing the block diagonal preconditioner for each 2D matrix.\n",
    "    \"\"\"\n",
    "    n, m, _ = A.shape\n",
    "    \n",
    "    # Initialize the preconditioner array\n",
    "    prec = np.zeros_like(A)\n",
    "    \n",
    "    for k in range(n):\n",
    "        # Find block start indices from binary predictions\n",
    "        block_starts = np.where(binary_predictions[k] == 1)[0]\n",
    "        \n",
    "        # Ensure to include the end of the matrix as a block end\n",
    "        block_starts = np.append(block_starts, m)\n",
    "        \n",
    "        # Extract diagonal blocks\n",
    "        blocks = []\n",
    "        for i in range(len(block_starts) - 1):\n",
    "            start = block_starts[i]\n",
    "            end = block_starts[i + 1]\n",
    "            blocks.append(A[k, start:end, start:end])\n",
    "        \n",
    "        # Invert diagonal blocks if they are invertible\n",
    "        inv_blocks = []\n",
    "        for block in blocks:\n",
    "            try:\n",
    "                inv_block = np.linalg.inv(block)\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Use pseudoinverse if the block is singular\n",
    "                inv_block = np.linalg.pinv(block)\n",
    "            inv_blocks.append(inv_block)\n",
    "        \n",
    "        # Create block diagonal matrix for the k-th matrix\n",
    "        for i, block in enumerate(inv_blocks):\n",
    "            start = block_starts[i]\n",
    "            end = block_starts[i + 1]\n",
    "            prec[k, start:end, start:end] = block\n",
    "    \n",
    "    return prec"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create preconditioner\n",
    "prec = block_jacobi_preconditioner_from_predictions(test_matrices, binary_predictions)\n",
    "\n",
    "print(\"Original Matrix A (first instance):\")\n",
    "print(A[0])\n",
    "print(\"\\nBlock Jacobi Preconditioner (first instance):\")\n",
    "print(prec[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMRES Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Condition Number before Preconditioning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(test_labels.shape)\n",
    "print(test_labels[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def check_condition_number(matrix):\n",
    "    try:\n",
    "        condition_number = np.linalg.cond(matrix)\n",
    "    except np.linalg.LinAlgError:\n",
    "        condition_number = float('inf')\n",
    "    return condition_number\n",
    "\n",
    "check_condition_number(test_matrices[0])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Convert test_labels to integer indices\n",
    "block_starts = np.where(test_labels[0] == 1)[0]\n",
    "\n",
    "# Check condition numbers\n",
    "for i, matrix in enumerate(test_matrices):\n",
    "    condition_number = check_condition_number(matrix)\n",
    "    print(f\"Condition number of matrix {i}: {condition_number}\")\n",
    "    block_starts = np.where(binary_predictions[i] == 1)[0]\n",
    "    block_starts = np.append(block_starts, matrix.shape[0])\n",
    "    for j in range(len(block_starts) - 1):\n",
    "        start = block_starts[j]\n",
    "        end = block_starts[j + 1]\n",
    "        block = matrix[start:end, start:end]\n",
    "        block_condition_number = check_condition_number(block)\n",
    "        print(f\"Condition number of block {j} of matrix {i}: {block_condition_number}\")\n",
    "        prec_block = prec[i, start:end, start:end]\n",
    "        prec_block_condition_number = check_condition_number(prec_block)\n",
    "        print(f\"Condition number of preconditioner block {j} of matrix {i}: {prec_block_condition_number}\")\n",
    "\n",
    "print(\"Original Matrix A (first instance):\")\n",
    "print(test_matrices[0])\n",
    "print(\"\\nBlock Jacobi Preconditioner (first instance):\")\n",
    "print(prec[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run GMRES solver on unconditioned matrices\n",
    "from scipy.sparse.linalg import gmres\n",
    "\n",
    "# Define parameters\n",
    "maxit = 100\n",
    "tol = 1e-5\n",
    "\n",
    "solutions_no_preconditioner = []  # List to store solution vectors for all matrices\n",
    "\n",
    "\n",
    "for i, matrix in enumerate(test_matrices):\n",
    "    # Convert matrix to sparse format\n",
    "    A_sparse = scipy.sparse.csr_matrix(matrix)\n",
    "\n",
    "    # Define right-hand side as a vector of ones\n",
    "    b = np.ones(matrix.shape[0])\n",
    "\n",
    "    # Extract the corresponding preconditioner for this matrix\n",
    "    preconditioner_matrix = scipy.sparse.csr_matrix(prec[i])\n",
    "\n",
    "    # Run GMRES solver\n",
    "    x, iteration_number = gmres(A_sparse, b, M=preconditioner_matrix, tol=tol, maxiter=maxit)\n",
    "    solutions_no_preconditioner.append(x)\n",
    "    print(f\"Matrix {i} solved in {iteration_number}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "solutions_with_preconditioner = []  \n",
    "\n",
    "for i, matrix in enumerate(test_matrices):\n",
    "    # Convert matrix to sparse format\n",
    "    A_sparse = scipy.sparse.csr_matrix(matrix)\n",
    "\n",
    "    # Define right-hand side as a vector of ones\n",
    "    b = np.ones(matrix.shape[0])\n",
    "\n",
    "    # Extract the corresponding preconditioner for this matrix\n",
    "    preconditioner_matrix = scipy.sparse.csr_matrix(prec[i])\n",
    "\n",
    "    # Run GMRES solver\n",
    "    x, exit_code = gmres(A_sparse, b, M=preconditioner_matrix, tol=1e-1, maxiter=5000)\n",
    "    solutions_with_preconditioner.append(x)\n",
    "    print(f\"Matrix {i} solved with exit code {exit_code}\")\n",
    "\n",
    "print(\"All matrices solved with preconditioner.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "while True:\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
