{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "current_base_path = os.getcwd()\n",
    "print(f\"Aktueller Arbeitsverzeichnis-Pfad: {current_base_path}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giJMRCShlduG"
   },
   "source": [
    "# Generate Synthetic Data\n",
    "Generate a set of `n` square symmetrical and positive semi matrices of dimensions `MATRIX_DIM` to RAM.\n",
    "\n",
    "## Define Constants\n",
    "The following constants are used throughout the rest of the notebook."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:41.732692Z",
     "start_time": "2024-06-09T14:16:41.725097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MATRIX_DIM = 64\n",
    "NUMBER_OF_MATRICES = 100\n",
    "\n",
    "AVERAGE_BLOCK_SIZE = 10\n",
    "BLOCK_SIZE_STD_DEV = 0.66\n",
    "\n",
    "NOISE_BACKGROUND_DENSITY_RANGE = (0.3, 0.5)\n",
    "NOISE_BACKGROUND_VALUE_RANGE = (0.0, 0.5)\n",
    "\n",
    "NOISE_BLOCK_GAP_CHANCE = 0.5\n",
    "NOISE_BLOCK_SIZE_RANGE = (3, 32)\n",
    "NOISE_BLOCK_DENSITY_RANGE = (0.3, 0.5)\n",
    "NOISE_BLOCK_VALUE_RANGE = (0.0, 1.0)\n",
    "\n",
    "TRUE_BLOCK_GAP_CHANCE = 0.0\n",
    "TRUE_BLOCK_SIZE_RANGE = (2, 32)\n",
    "TRUE_BLOCK_DENSITY_RANGE = (0.5, 0.7)\n",
    "TRUE_BLOCK_VALUE_RANGE = (0.0, 1.0)\n",
    "\n",
    "DIAGONAL_BAND_RADIUS = 10\n",
    "\n",
    "RNG_SEED = 42"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:52.101965Z",
     "start_time": "2024-06-09T14:16:44.878043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "import matrixlib.io\n",
    "import matrixlib.util\n",
    "import matrixlib.plot\n",
    "import matrixlib.generate\n",
    "from matrixlib.metadata import MatrixMetadata\n",
    "\n",
    "#import CNN.dataloader\n",
    "import CNN.models\n",
    "import CNN.training\n",
    "import CNN.evaluation\n",
    "\n",
    "# reload the library when changes were made to it\n",
    "importlib.reload(matrixlib.io)\n",
    "importlib.reload(matrixlib.util)\n",
    "importlib.reload(matrixlib.plot)\n",
    "importlib.reload(matrixlib.generate)\n",
    "#importlib.reload(CNN.dataloader)\n",
    "\n",
    "importlib.reload(CNN.models)\n",
    "importlib.reload(CNN.training)\n",
    "importlib.reload(CNN.evaluation)\n",
    "importlib.reload(matrixlib.generate)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 16:16:46.890858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-09 16:16:49.635241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'matrixlib.generate' from '/home/moonchild/PycharmProjects/opencampus-preconditioner-ai-project/matrixlib/generate.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:52.114684Z",
     "start_time": "2024-06-09T14:16:52.106309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_matrices(number_of_matrices: int, dimension: int) -> (np.ndarray, np.ndarray):\n",
    "    generated_metadata: MatrixMetadata = MatrixMetadata(NUMBER_OF_MATRICES, MATRIX_DIM)\n",
    "    generated_matrices: np.array = matrixlib.generate.__init_zero_matrices(number_of_matrices, dimension)\n",
    "\n",
    "    # add background noise to the generated matrices\n",
    "    matrixlib.generate.add_noise(\n",
    "        generated_matrices,\n",
    "        generated_metadata.noise_background_density,\n",
    "        NOISE_BACKGROUND_DENSITY_RANGE,\n",
    "        NOISE_BACKGROUND_VALUE_RANGE\n",
    "    )\n",
    "\n",
    "    # add noise blocks with higher values to the diagonal of the matrix\n",
    "    noise_block_sizes = matrixlib.generate.add_blocks(\n",
    "        generated_matrices,\n",
    "        None,\n",
    "        generated_metadata.noise_block_density,\n",
    "        NOISE_BLOCK_DENSITY_RANGE,\n",
    "        NOISE_BLOCK_VALUE_RANGE,\n",
    "        NOISE_BLOCK_SIZE_RANGE,\n",
    "        NOISE_BLOCK_GAP_CHANCE,\n",
    "        AVERAGE_BLOCK_SIZE,\n",
    "        BLOCK_SIZE_STD_DEV,\n",
    "    )\n",
    "\n",
    "    # add 'real' blocks to the diagonal of the matrix without gaps\n",
    "    true_block_sizes = matrixlib.generate.add_blocks(\n",
    "        generated_matrices,\n",
    "        generated_metadata.block_starts,\n",
    "        generated_metadata.true_block_density,\n",
    "        TRUE_BLOCK_DENSITY_RANGE,\n",
    "        TRUE_BLOCK_VALUE_RANGE,\n",
    "        TRUE_BLOCK_SIZE_RANGE,\n",
    "        TRUE_BLOCK_GAP_CHANCE,\n",
    "        AVERAGE_BLOCK_SIZE,\n",
    "        BLOCK_SIZE_STD_DEV,\n",
    "    )\n",
    "\n",
    "    return generated_matrices, generated_metadata"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:55.119616Z",
     "start_time": "2024-06-09T14:16:53.620687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "matrices, metadata = generate_matrices(NUMBER_OF_MATRICES, MATRIX_DIM)\n",
    "matrices = matrices.reshape(NUMBER_OF_MATRICES, MATRIX_DIM, MATRIX_DIM, 1)\n",
    "labels = metadata.block_starts\n",
    "\n",
    "print(matrices.shape)\n",
    "\n",
    "# Create dataset from matrices and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((matrices, labels))\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * NUMBER_OF_MATRICES)\n",
    "val_size = int(0.1 * NUMBER_OF_MATRICES)\n",
    "test_size = NUMBER_OF_MATRICES - train_size - val_size\n",
    "\n",
    "train_dataset = dataset.take(train_size).shuffle(buffer_size=10)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "test_dataset = dataset.skip(train_size + val_size)\n",
    "\n",
    "print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating matrices with a total number of 409600 entries (100 64x64 matrices)\n",
      "Matrix for element size 8 bytes is a total of 3276800 bytes (3.125 MiB)\n",
      "(100, 64, 64, 1)\n",
      "Train size: 80, Val size: 10, Test size: 10\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(train_dataset)\n",
    "print(val_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for element in val_dataset:\n",
    "    print(element)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# training loop\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = CNN.models.Baseline(input_shape=(MATRIX_DIM, MATRIX_DIM, 1))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=BinaryCrossentropy(),  # Use a specific instance if configuring parameters\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 10\n",
    "log_dir = \"runs/matrix_cnn_experiment\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "print(\"Files in log directory:\", os.listdir(log_dir))\n",
    "\n",
    "trained_model, train_losses, val_losses = CNN.training.train_model(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset.batch(1),  # Adjust batch size as needed\n",
    "    val_dataset=val_dataset.batch(1),\n",
    "    loss_fn=BinaryCrossentropy(),  # Using an instance allows more configuration if needed\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    num_epochs=num_epochs,\n",
    "    log_dir=log_dir\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# backup\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_params = {\n",
    "    'batch_size': 4,\n",
    "    'img_size': (64, 64)\n",
    "}\n",
    "\n",
    "dataset = CNN.dataloader.MatrixDataset(**data_params) \n",
    "train_dataset = dataset._create_dynamic_dataset(number_of_matrices=NUMBER_OF_MATRICES, dimension=MATRIX_DIM, split='train')\n",
    "val_dataset = dataset._create_dynamic_dataset(number_of_matrices=NUMBER_OF_MATRICES, dimension=MATRIX_DIM, split='val')\n",
    "test_dataset = dataset._create_dynamic_dataset(number_of_matrices=NUMBER_OF_MATRICES, dimension=MATRIX_DIM, split='test')\n",
    "\n",
    "for data, labels in train_dataset:\n",
    "    print(data.numpy(), labels.numpy())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for data, labels in val_dataset:\n",
    "    print(data.numpy(), labels.numpy())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(val_dataset)\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# train"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "model = CNN.models.Baseline\n",
    "loss_fn = tf.keras.losses.binary_crossentropy\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "log_dir = \"runs/matrix_cnn_experiment\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "print(\"Files in log directory:\", os.listdir(log_dir))\n",
    "\n",
    "CNN.training.train_model(model, train_dataset, val_dataset, loss_fn, optimizer, num_epochs=10, log_dir=log_dir)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Matrix Venv",
   "language": "python",
   "name": "matrix_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
