{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T22:42:48.623966Z",
     "start_time": "2024-06-08T22:42:41.215551Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 00:42:42.543040: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-09 00:42:42.912614: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-09 00:42:43.757119: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-09 00:42:46.314676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train_model(model, train_dataset, val_dataset, loss_fn, optimizer, num_epochs, log_dir):\n",
    "    \"\"\"\n",
    "    Trains the given model using the provided training and validation datasets, loss function, and optimizer.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to be trained.\n",
    "        train_dataset (tf.data.Dataset): The dataset for training the model.\n",
    "        val_dataset (tf.data.Dataset): The dataset for validating the model during training.\n",
    "        loss_fn (tf.keras.losses.Loss): The loss function used for training.\n",
    "        optimizer (tf.keras.optimizers.Optimizer): The optimizer used for training.\n",
    "        num_epochs (int): The number of epochs to train the model.\n",
    "        log_dir (str): Directory for storing TensorBoard logs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained model, a list of training losses, and a list of validation losses.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Creates a TensorBoard writer for logging.\n",
    "    2. Defines early stopping and model checkpoint callbacks.\n",
    "    3. Initializes lists to track training and validation losses.\n",
    "    4. Iterates over the number of epochs:\n",
    "       - Trains the model for one epoch and logs the training loss.\n",
    "       - Validates the model and logs the validation loss.\n",
    "       - Prints the training and validation losses for each epoch.\n",
    "       - Applies early stopping and saves the best model checkpoint.\n",
    "    5. If early stopping is triggered, stops training.\n",
    "    6. Saves the final model weights.\n",
    "    7. Returns the trained model, training losses, and validation losses.\n",
    "    \"\"\"\n",
    "    # Create TensorBoard writer\n",
    "    writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    # Define early stopping and model checkpoint\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(filepath=\"best_model_checkpoint.keras\", monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    # Initialize lists to track losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        epoch_train_loss = train_one_epoch(model, train_dataset, loss_fn, optimizer)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Logging training loss\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('Loss/Train', epoch_train_loss, step=epoch)\n",
    "\n",
    "        # Validation phase\n",
    "        epoch_val_loss = validate_one_epoch(model, val_dataset, loss_fn)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        # Logging validation loss\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('Loss/Validation', epoch_val_loss, step=epoch)\n",
    "\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f} | Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping and checkpointing\n",
    "        early_stopping.on_epoch_end(epoch, logs={'val_loss': epoch_val_loss})\n",
    "        checkpoint.on_epoch_end(epoch, logs={'val_loss': epoch_val_loss})\n",
    "\n",
    "        if early_stopping.stopped_epoch > 0:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    model.save_weights(\"best_model.keras\")\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Define function to train for one epoch\n",
    "def train_one_epoch(model, train_dataset, loss_fn, optimizer):\n",
    "     \"\"\"\n",
    "    Trains the model for one epoch using the provided training dataset, loss function, and optimizer.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to be trained.\n",
    "        train_dataset (tf.data.Dataset): The dataset for training the model.\n",
    "        loss_fn (tf.keras.losses.Loss): The loss function used for training.\n",
    "        optimizer (tf.keras.optimizers.Optimizer): The optimizer used for training.\n",
    "\n",
    "    Returns:\n",
    "        float: The average training loss for the epoch.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Initializes the epoch's training loss to zero.\n",
    "    2. Iterates over the training dataset:\n",
    "       - Performs a forward pass of the model.\n",
    "       - Calculates the loss between the model outputs and true labels.\n",
    "       - Computes the gradients of the loss with respect to the model's trainable variables.\n",
    "       - Applies the gradients to update the model's weights.\n",
    "       - Accumulates the batch loss to the epoch's total loss.\n",
    "    3. Averages the total loss over all batches to obtain the epoch's average training loss.\n",
    "    4. Returns the average training loss for the epoch.\n",
    "    \"\"\"\n",
    "     epoch_train_loss = 0.0\n",
    "     \n",
    "     for features, labels in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(features, training=True)\n",
    "            loss = loss_fn(labels, outputs)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        epoch_train_loss += loss.numpy() * features.shape[0]\n",
    "\n",
    "     epoch_train_loss /= len(train_dataset)\n",
    "     return epoch_train_loss\n",
    "\n",
    "# Define function to validate for one epoch\n",
    "def validate_one_epoch(model, val_dataset, loss_fn):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch using the provided validation dataset and loss function.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to be validated.\n",
    "        val_dataset (tf.data.Dataset): The dataset for validating the model.\n",
    "        loss_fn (tf.keras.losses.Loss): The loss function used for validation.\n",
    "\n",
    "    Returns:\n",
    "        float: The average validation loss for the epoch.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Initializes the epoch's validation loss to zero.\n",
    "    2. Iterates over the validation dataset:\n",
    "       - Performs a forward pass of the model in inference mode.\n",
    "       - Calculates the loss between the model outputs and true labels.\n",
    "       - Accumulates the batch loss to the epoch's total loss.\n",
    "    3. Averages the total loss over all batches to obtain the epoch's average validation loss.\n",
    "    4. Returns the average validation loss for the epoch.\n",
    "    \"\"\"\n",
    "    epoch_val_loss = 0.0\n",
    "\n",
    "    for features, labels in val_dataset:\n",
    "        outputs = model(features, training=False)\n",
    "        loss = loss_fn(labels, outputs)\n",
    "        \n",
    "        epoch_val_loss += loss.numpy() * features.shape[0]\n",
    "\n",
    "    epoch_val_loss /= len(val_dataset)\n",
    "    return epoch_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_dataset, loss_fn):\n",
    "    # Initialize variables to store loss and other metrics\n",
    "    test_loss = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    # Lists to store true and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Iterate through the test dataset\n",
    "    for features, labels in test_dataset:\n",
    "        # Get model predictions\n",
    "        outputs = model(features, training=False)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(labels, outputs)\n",
    "        \n",
    "        # Aggregate the loss\n",
    "        test_loss += loss.numpy() * features.shape[0]\n",
    "        num_samples += features.shape[0]\n",
    "\n",
    "        # Convert predictions to class labels\n",
    "        predicted_labels.extend(tf.argmax(outputs, axis=1).numpy())\n",
    "        true_labels.extend(tf.argmax(labels, axis=1).numpy())\n",
    "    \n",
    "    # Average loss over all samples\n",
    "    average_test_loss = test_loss / num_samples\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"Test Loss: {average_test_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'loss': average_test_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web3venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
