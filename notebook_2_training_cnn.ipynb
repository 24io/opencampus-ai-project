{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T10:05:18.989785Z",
     "start_time": "2024-07-31T10:05:09.814430Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import datetime\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Modellib\n",
    "import modellib.cnn\n",
    "import modellib.train\n",
    "import modellib.io as io\n",
    "import modellib.evaluate as eval"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 12:05:11.624646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 12:05:11.862289: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 12:05:11.912035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and transform matrix datasets"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T10:05:23.312512Z",
     "start_time": "2024-07-31T10:05:23.182779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reading the datasets\n",
    "train_bands, train_labels = io.read_from_hdf5(\"data/datasets/train_dataset_64_1600.h5\")\n",
    "val_bands, val_labels = io.read_from_hdf5(\"data/datasets/val_dataset_64_200.h5\")\n",
    "test_bands, test_labels = io.read_from_hdf5(\"data/datasets/test_dataset_64_200.h5\")\n",
    "\n",
    "# Printing shapes to verify\n",
    "print(f\"Train bands shape: {train_bands.shape}, Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Validation bands shape: {val_bands.shape}, Validation labels shape: {val_labels.shape}\")\n",
    "print(f\"Test bands shape: {test_bands.shape}, Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Convert to tensorflow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_bands, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_bands, val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_bands, test_labels))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bands shape: (1600, 21, 64, 1), Train labels shape: (1600, 64)\n",
      "Validation bands shape: (200, 21, 64, 1), Validation labels shape: (200, 64)\n",
      "Test bands shape: (200, 21, 64, 1), Test labels shape: (200, 64)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "run tensorboard --logdir logs to launch tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T10:05:28.329565Z",
     "start_time": "2024-07-31T10:05:28.308447Z"
    }
   },
   "source": [
    "# Create log dir\n",
    "log_dir = \"logs/cnn/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "print(\"Files in log directory:\", os.listdir(log_dir))\n",
    "\n",
    "# Create Learning Rate scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.00001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 16\n",
    "num_epochs = 200\n",
    "input_shape = (21, 64, 1)\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=lr_schedule)\n",
    "class_weights = {0: 0.2, 1: 0.8}"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in log directory: []\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create and Compile Model\n",
    "model = modellib.cnn.create_compile_model_custom_loss(\n",
    "    input_shape, \n",
    "    optimizer, \n",
    "    class_weights\n",
    ")\n",
    "\n",
    "# Start Training Loop\n",
    "trained_model, train_losses, val_losses = modellib.train.train_model(\n",
    "    model,\n",
    "    train_dataset.batch(batch_size),  \n",
    "    val_dataset.batch(batch_size),\n",
    "    num_epochs,\n",
    "    log_dir\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot losses\n",
    "modellib.train.plot_losses(train_losses, val_losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluation_results = modellib.evaluate.evaluate_model(\n",
    "    trained_model,\n",
    "    test_dataset.batch(batch_size),\n",
    "    class_weights\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Restore Best Model Weights\n",
    "\n",
    "### *IMPORTANT: The model weights can only be imported when using tensorflow 2.15.0. with keras 2.0.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T10:05:42.791320Z",
     "start_time": "2024-07-31T10:05:40.053315Z"
    }
   },
   "source": [
    "# Restore weights from best run\n",
    "new_model = modellib.cnn.Baseline(input_shape)\n",
    "new_model.build((None,) + input_shape)  # None represents the batch dimension\n",
    "new_model.load_weights(\"data/weights/best_cnn.weights.h5\")\n",
    "\n",
    "# print weights\n",
    "print(f\"Model Weights: {new_model.get_weights()}\")\n",
    "\n",
    "# Evaluate the restored model\n",
    "new_results = modellib.evaluate.evaluate_model(\n",
    "    new_model,\n",
    "    test_dataset.batch(batch_size),\n",
    "    class_weights\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights: []\n",
      "Test Loss: 0.1944\n",
      "Element-wise Accuracy: 0.55921875\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    no block       0.89      0.58      0.70     11516\n",
      "       block       0.09      0.37      0.14      1284\n",
      "\n",
      "    accuracy                           0.56     12800\n",
      "   macro avg       0.49      0.47      0.42     12800\n",
      "weighted avg       0.81      0.56      0.65     12800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6686 4830]\n",
      " [ 812  472]]\n",
      "True Positives: 472\n",
      "False Negatives: 812\n",
      "True Negatives: 6686\n",
      "False Positives: 4830\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Make Predictions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "importlib.reload(eval)\n",
    "test_predictions = new_model.predict(test_dataset.batch(batch_size))\n",
    "\n",
    "# Convert to Binary\n",
    "threshold = 0.5\n",
    "binary_predictions = (test_predictions >= threshold).astype(int)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save predictions\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save blockstarts \n",
    "# np.save('data/blockstarts/cnn_predictions.npy', binary_predictions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load blockstarts and print to verify\n",
    "loaded_predictions = np.load('data/blockstarts/cnn_predictions.npy')\n",
    "print(loaded_predictions)\n",
    "\n",
    "# Evaluate to verify\n",
    "metrics = eval.calculate_metrics(test_labels, loaded_predictions)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
